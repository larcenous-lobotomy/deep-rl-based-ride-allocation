{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mABCNhyJsT-B"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from collections import deque\n",
        "import collections\n",
        "import pickle\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from itertools import permutations\n",
        "\n",
        "# for building DQN model\n",
        "!pip install -q keras\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# for plotting graphs\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzHWOOrT8ovd",
        "outputId": "94a5306d-8794-49d0-a2fa-f261f4f67b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH_vaD7Ls6uS"
      },
      "outputs": [],
      "source": [
        "# Defining hyperparameters\n",
        "m = 5  # number of cities, ranges from 0 ..... m-1\n",
        "t = 24  # number of hours, ranges from 0 .... t-1\n",
        "d = 7  # number of days, ranges from 0 ... d-1\n",
        "C = 5  # Per hour fuel and other costs\n",
        "R = 9  # per hour revenue from a passenger\n",
        "\n",
        "Time_matrix = np.load(\"/content/drive/MyDrive/TM.npy\")\n",
        "\n",
        "\n",
        "class CabDriver():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.action_space = [(0, 0)] + \\\n",
        "            list(permutations([i for i in range(m)], 2))\n",
        "        self.state_space = [[x, y, z]\n",
        "                            for x in range(m) for y in range(t) for z in range(d)]\n",
        "        self.state_init = random.choice(self.state_space)\n",
        "        #self.state_init = [0,0,0]\n",
        "        # Start the first round\n",
        "        self.reset()\n",
        "\n",
        "    def state_encod_arch1(self, state):\n",
        "        state_encod = [0 for _ in range(m+t+d)]\n",
        "        state_encod[self.state_get_loc(state)] = 1\n",
        "        state_encod[m+self.state_get_time(state)] = 1\n",
        "        state_encod[m+t+self.state_get_day(state)] = 1\n",
        "\n",
        "        return state_encod\n",
        "\n",
        "    ## Getting number of requests\n",
        "\n",
        "    def requests(self, state):\n",
        "        location = state[0]\n",
        "        if location == 0:\n",
        "            requests = np.random.poisson(2)\n",
        "        if location == 1:\n",
        "            requests = np.random.poisson(12)\n",
        "        if location == 2:\n",
        "            requests = np.random.poisson(4)\n",
        "        if location == 3:\n",
        "            requests = np.random.poisson(7)\n",
        "        if location == 4:\n",
        "            requests = np.random.poisson(8)\n",
        "\n",
        "        if requests > 15:\n",
        "            requests = 15\n",
        "        # (0,0) is not considered as customer request, however the driver is free to refuse all\n",
        "        # customer requests. Hence, add the index of action (0,0).\n",
        "        possible_actions_index = random.sample(range(1, (m-1)*m + 1), requests) + [0]\n",
        "        actions = [self.action_space[i] for i in possible_actions_index]\n",
        "\n",
        "        return possible_actions_index, actions\n",
        "\n",
        "    def update_time_day(self, time, day, ride_duration):\n",
        "        ride_duration = int(ride_duration)\n",
        "\n",
        "        if (time + ride_duration) < 24:\n",
        "            time = time + ride_duration\n",
        "            # day is unchanged\n",
        "        else:\n",
        "            # duration taken spreads over to subsequent days\n",
        "            # convert the time to 0-23 range\n",
        "            time = (time + ride_duration) % 24 \n",
        "            \n",
        "            # Get the number of days\n",
        "            num_days = (time + ride_duration) // 24\n",
        "            \n",
        "            # Convert the day to 0-6 range\n",
        "            day = (day + num_days ) % 7\n",
        "\n",
        "        return time, day\n",
        "    \n",
        "    def next_state_func(self, state, action, Time_matrix):\n",
        "        next_state = []\n",
        "        \n",
        "        # Initialize various times\n",
        "        total_time   = 0\n",
        "        transit_time = 0    # to go from current  location to pickup location\n",
        "        wait_time    = 0    # in case driver chooses to refuse all requests\n",
        "        ride_time    = 0    # from Pick-up to drop\n",
        "        \n",
        "        # Derive the current location, time, day and request locations\n",
        "        curr_loc = self.state_get_loc(state)\n",
        "        pickup_loc = self.action_get_pickup(action)\n",
        "        drop_loc = self.action_get_drop(action)\n",
        "        curr_time = self.state_get_time(state)\n",
        "        curr_day = self.state_get_day(state)\n",
        "        \"\"\"\n",
        "         3 Scenarios: \n",
        "           a) Refuse all requests\n",
        "           b) Driver is already at pick up point\n",
        "           c) Driver is not at the pickup point.\n",
        "        \"\"\"    \n",
        "        if ((pickup_loc== 0) and (drop_loc == 0)):\n",
        "            # Refuse all requests, so wait time is 1 unit, next location is current location\n",
        "            wait_time = 1\n",
        "            next_loc = curr_loc\n",
        "        elif (curr_loc == pickup_loc):\n",
        "            # means driver is already at pickup point, wait and transit are both 0 then.\n",
        "            ride_time = Time_matrix[curr_loc][drop_loc][curr_time][curr_day]\n",
        "            \n",
        "            # next location is the drop location\n",
        "            next_loc = drop_loc\n",
        "        else:\n",
        "            # Driver is not at the pickup point, he needs to travel to pickup point first\n",
        "            # time take to reach pickup point\n",
        "            transit_time      = Time_matrix[curr_loc][pickup_loc][curr_time][curr_day]\n",
        "            new_time, new_day = self.update_time_day(curr_time, curr_day, transit_time)\n",
        "            \n",
        "            # The driver is now at the pickup point\n",
        "            # Time taken to drop the passenger\n",
        "            ride_time = Time_matrix[pickup_loc][drop_loc][new_time][new_day]\n",
        "            next_loc  = drop_loc\n",
        "\n",
        "        # Calculate total time as sum of all durations\n",
        "        total_time = (wait_time + transit_time + ride_time)\n",
        "        next_time, next_day = self.update_time_day(curr_time, curr_day, total_time)\n",
        "        \n",
        "        # Construct next_state using the next_loc and the new time states.\n",
        "        next_state = [next_loc, next_time, next_day]\n",
        "        \n",
        "        return next_state, wait_time, transit_time, ride_time\n",
        "    \n",
        "\n",
        "    def reset(self):\n",
        "        return self.action_space, self.state_space, self.state_init\n",
        "\n",
        "    def reward_func(self, wait_time, transit_time, ride_time):\n",
        "        # transit and wait time yield no revenue, only battery costs, so they are idle times.\n",
        "        passenger_time = ride_time\n",
        "        idle_time      = wait_time + transit_time\n",
        "        \n",
        "        reward = (R * passenger_time) - (C * (passenger_time + idle_time))\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def step(self, state, action, Time_matrix):\n",
        "        # Get the next state and the various time durations\n",
        "        next_state, wait_time, transit_time, ride_time = self.next_state_func(\n",
        "            state, action, Time_matrix)\n",
        "\n",
        "        # Calculate the reward based on the different time durations\n",
        "        rewards = self.reward_func(wait_time, transit_time, ride_time)\n",
        "        total_time = wait_time + transit_time + ride_time\n",
        "        \n",
        "        return rewards, next_state, total_time\n",
        "\n",
        "    def state_get_loc(self, state):\n",
        "        return state[0]\n",
        "\n",
        "    def state_get_time(self, state):\n",
        "        return state[1]\n",
        "\n",
        "    def state_get_day(self, state):\n",
        "        return state[2]\n",
        "\n",
        "    def action_get_pickup(self, action):\n",
        "        return action[0]\n",
        "\n",
        "    def action_get_drop(self, action):\n",
        "        return action[1]\n",
        "\n",
        "    def state_set_loc(self, state, loc):\n",
        "        state[0] = loc\n",
        "\n",
        "    def state_set_time(self, state, time):\n",
        "        state[1] = time\n",
        "\n",
        "    def state_set_day(self, state, day):\n",
        "        state[2] = day\n",
        "\n",
        "    def action_set_pickup(self, action, pickup):\n",
        "        action[0] = pickup\n",
        "\n",
        "    def action_set_drop(self, action, drop):\n",
        "        action[1] = drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaXlVAGasT-V"
      },
      "outputs": [],
      "source": [
        "#Defining a function to save the Q-dictionary as a pickle file\n",
        "def save_obj(obj, name ):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h8AkwbgsT-Z"
      },
      "source": [
        "### Agent class for Deep Q-Learning Network "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTjTTtsasT-d"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \n",
        "        # Define size of state and action\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Write here: Specify the hyper parameters for the DQN\n",
        "        self.discount_factor = 0.93\n",
        "        self.learning_rate = 0.02\n",
        "        self.epsilon = 1\n",
        "        self.epsilon_max = 1\n",
        "        self.epsilon_decay = -0.003 \n",
        "        self.epsilon_min = 0.0001\n",
        "        \n",
        "        self.batch_size = 20\n",
        "\n",
        "        # create replay memory using deque\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        # Initialize the value of the states tracked\n",
        "        self.states_tracked = []\n",
        "        \n",
        "        # We are going to track state [0,0,0] and action (0,2) at index 2 in the action space.\n",
        "        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n",
        "\n",
        "        # create main model and target model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    # approximate Q function using Neural Network\n",
        "    def build_model(self):\n",
        "        input_shape = self.state_size\n",
        "        model = Sequential()\n",
        "        # Write your code here: Add layers to your neural nets       \n",
        "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "        # the output layer: output is of size num_actions\n",
        "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        model.summary\n",
        "        return model\n",
        "\n",
        "    def get_action(self, state, possible_actions_index, actions):\n",
        "        # get action from model using epsilon-greedy policy\n",
        "        # Decay in Îµ after each episode       \n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            # explore: choose a random action from the ride requests\n",
        "            return random.choice(possible_actions_index)\n",
        "        else:\n",
        "            # choose the action with the highest q(s, a)\n",
        "            # the first index corresponds to the batch size, so\n",
        "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
        "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
        "\n",
        "            # Use the model to predict the Q_values.\n",
        "            q_value = self.model.predict(state)\n",
        "\n",
        "            # truncate the array to only those actions that are part of the ride  requests.\n",
        "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
        "\n",
        "            return possible_actions_index[np.argmax(q_vals_possible)]\n",
        "\n",
        "    def append_sample(self, state, action_index, reward, next_state, done):\n",
        "        self.memory.append((state, action_index, reward, next_state, done))\n",
        "        \n",
        "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
        "    def train_model(self):\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            # Sample batch from the memory\n",
        "            mini_batch = random.sample(self.memory, self.batch_size)\n",
        "            # initialise two matrices - update_input and update_output\n",
        "            update_input = np.zeros((self.batch_size, self.state_size))\n",
        "            update_output = np.zeros((self.batch_size, self.state_size))\n",
        "            actions, rewards, done = [], [], []\n",
        "\n",
        "            # populate update_input and update_output and the lists rewards, actions, done\n",
        "            for i in range(self.batch_size):\n",
        "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
        "                update_input[i] = env.state_encod_arch1(state)     \n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                update_output[i] = env.state_encod_arch1(next_state)\n",
        "                done.append(done_boolean)\n",
        "\n",
        "            # predict the target q-values from states s\n",
        "            target = self.model.predict(update_input)\n",
        "            # target for q-network\n",
        "            target_qval = self.model.predict(update_output)\n",
        "\n",
        "\n",
        "            # update the target values\n",
        "            for i in range(self.batch_size):\n",
        "                if done[i]:\n",
        "                    target[i][actions[i]] = rewards[i]\n",
        "                else: # non-terminal state\n",
        "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
        "            # model fit\n",
        "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
        "            \n",
        "    def save_tracking_states(self):\n",
        "        # Use the model to predict the q_value of the state \n",
        "        q_value = self.model.predict(self.track_state)\n",
        "        \n",
        "        print(\"States_tracked value {0}.\".format(q_value[0][2]))\n",
        "        \n",
        "        # Grab the q_value of the action index \n",
        "        self.states_tracked.append(q_value[0][2])\n",
        "        \n",
        "    def save(self, name):\n",
        "        self.model.save(name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ln4R3QPt9kW"
      },
      "source": [
        "### Instantiate Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuzRZZx-okGO",
        "outputId": "0eef84de-ee8b-47d6-c84e-e0f3f5dfb596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "episode_time = 24*30 #30 days before which car has to be recharged\n",
        "n_episodes = 1500\n",
        "m = 5\n",
        "t = 24\n",
        "d = 7\n",
        "\n",
        "# Invoke Env class\n",
        "env = CabDriver()\n",
        "action_space, state_space, state = env.reset()\n",
        "\n",
        "# Set up state and action sizes.\n",
        "state_size = m+t+d\n",
        "action_size = len(action_space)\n",
        "\n",
        "# Invoke agent class\n",
        "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
        "\n",
        "# to store rewards in each episode\n",
        "rewards_per_episode, episodes = [], []\n",
        "# Rewards for state [0,0,0] being tracked.\n",
        "rewards_init_state = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iGWs-vqsT-k"
      },
      "source": [
        "### Training Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKWdnXvOsT-m"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "score_tracked = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "\n",
        "    done = False\n",
        "    score = 0\n",
        "    track_reward = False\n",
        "\n",
        "    # reset at the start of each episode\n",
        "    env = CabDriver()\n",
        "    action_space, state_space, state = env.reset()\n",
        "    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n",
        "    initial_state = env.state_init\n",
        "\n",
        "    # Total time driver rode in this episode\n",
        "    total_time = 0  \n",
        "    while not done:\n",
        "        # 1. Get a list of the ride requests driver got.\n",
        "        possible_actions_indices, actions = env.requests(state)\n",
        "        # 2. Pick epsilon-greedy action from possible actions for the current state.\n",
        "        action = agent.get_action(state, possible_actions_indices, actions)\n",
        "\n",
        "        # 3. Evaluate your reward and next state\n",
        "        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n",
        "        # 4. Total time driver rode in this episode\n",
        "        total_time += step_time\n",
        "        if (total_time > episode_time):\n",
        "            # if ride does not complete in stipulated time skip\n",
        "            # it and move to next episode.\n",
        "            done = True\n",
        "        else:\n",
        "            # 5. Append the experience to the memory\n",
        "            agent.append_sample(state, action, reward, next_state, done)\n",
        "            # 6. Train the model by calling function agent.train_model\n",
        "            agent.train_model()\n",
        "            # 7. Keep a track of rewards, Q-values, loss\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "    # store total reward obtained in this episode\n",
        "    rewards_per_episode.append(score)\n",
        "    episodes.append(episode)\n",
        "    \n",
        "\n",
        "    # epsilon decay\n",
        "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
        "\n",
        "    # every 10 episodes:\n",
        "    if ((episode + 1) % 10 == 0):\n",
        "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n",
        "                                                                         score,\n",
        "                                                                         len(agent.memory),\n",
        "                                                                         agent.epsilon, total_time))\n",
        "    # Save the Q_value of the state, action pair we are tracking\n",
        "    if ((episode + 1) % 5 == 0):\n",
        "        agent.save_tracking_states()\n",
        "\n",
        "    # Total rewards per episode\n",
        "    score_tracked.append(score)\n",
        "\n",
        "    if(episode % 1000 == 0):\n",
        "        print(\"Saving Model {}\".format(episode))\n",
        "        agent.save(name=\"car_model.h5\")\n",
        "\n",
        "    \n",
        "elapsed_time = time.time() - start_time\n",
        "print(elapsed_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXhqoXiQsT-p"
      },
      "outputs": [],
      "source": [
        "def save_obj(obj, name ):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "save_obj(agent.states_tracked,\"model_weights\")\n",
        "files.download(\"model_weights.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g_ZB9gbsT-r"
      },
      "source": [
        "### Tracking Convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmL5NV2tsT-t"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(16,7))\n",
        "plt.title('Q_value for state [0,0,0]  action (0,2)')\n",
        "xaxis = np.asarray(range(0, len(agent.states_tracked)))\n",
        "plt.semilogy(xaxis,np.asarray(agent.states_tracked))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "deep-rl-for-ride-allocation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}